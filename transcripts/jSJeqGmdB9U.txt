so it's just for context everyone like
I'm you know I have the baby bit uh logo
up here because you know put yourself in
my shoes really want you to live this
experience with me you know I'm young
into my career I'm not really great
confident in my skill set but you know
I'm excited and I've got my manager that
approaches me and says like when we have
this process that we want you to build a
solution end to end he didn't say you
have to use a specific service you got
to do a specific way I was kind of just
tasks with this right so you know here
yeah uh very grateful for the
opportunity excited nervous in things
right so this is the context of of this
project right so let's talk a little bit
about what the actual problem was I
needed to be automated so I had two
colleagues uh that were spending their
mornings manually submitting files to a
website and I've highlighted here words
that I think are are
sort of key to this problem here so they
were spending their mornings manually
submitting files to a website and this
combined three hours between both of
them every single day and six days a
week because this is a process that
needed to be done Monday through
Saturday this was in the energy industry
I don't know if anyone has any
experience working with the energy
industry but they have all sorts of
rules limitations very old processes as
well things that have to be submitted
it's weird because there's some stuff
that has to be submitted like exactly 9
59 in the morning and if it's one minute
after it's considered the next day a
bunch of different things so if anyone
has any uh experience in the industry in
the energy industry let me know in the
chat but anyway so the total was 18
hours of manual work a week right and
I've actually got here a representation
of my frustrated colleagues uh that you
know they just wanted to spend their
time this 18 hours of time that they
have per week doing things where they're
actually working on the skill set that
is specific to their role and not just
doing something that you could automate
something that was manual something that
was really sort of boring in one right
wanted to spend their time more
mindfully and I think this is also
important when we think about automation
because when we when we think automation
we think people lose their job but if
you work at the right place you've got
the right team around you you've got the
right management and Leadership they can
turn that time that was saved into more
mindful work for your team which is what
ended up happening in this process which
is great
um but anyway now let's talk a little
bit about the approach again still you
know young Gwen uh early in career Still
Still baby bit here but I through
conversation I sat down with the team
who were actually doing this process and
I'd figure out okay there's some key
facts here that makes me think that I
should use a serverless approach here so
anyway first of all again this is our
our team here sitting in a co-working
environment but anyway first of all the
first the whole process I knew for each
file to be submitted uh would take a few
minutes so it wasn't a long run it
wasn't something that needed to run for
days or hours or anything like that so
from them getting the file to submitting
it to this website it was a couple of
minutes so that's a key fact there
then I also knew that these files could
be processed independently so none of
these files appended it depended on each
other it was one file uploads submit
done move on to the next file which is
important I think there was at that
point when I first created the solution
from 50 to 60 files but the goal was to
have more and more files depending on
more clients coming in and whatnot but
again order didn't matter they didn't
have to be done together they just had
to be processed at some point important
here too
uh another key factor that I that I
found out was that each file had a
username and a password that rotates
each 90 days so in order for whatever
process that I was going to create it
needed to be able to securely access
these credentials and these credentials
also needed to be in a place that we
could in a streamlined way update them
because they need to be rotated every 90
days right so that's also important here
then I knew that the files that actually
get uploaded were generated by an
existing system and then these analysts
my colleagues were downloading the files
from the existing system manually but I
spoke to the lead engineer on that
project and then that he essentially
told me like yeah we're just sort of
generating them and not putting them
anywhere but if you gave me a place
where you wanted them to go get saved I
could do that for you so I was like okay
cool key fact right there too so I know
I could get the files to get once they
were generated to go somewhere where I
needed them to go
very very important part is the website
where we were actually uploading these
files to had an API so if you are a
developer you do any engineering work
you know apis are important when it
comes to automating and having systems
communicate with each other if it had an
API we were probably looking at using
like um what's that technology that you
can uh or that service that you can use
to automate like clicks in a website
there's I can't remember oh yeah there
is something it's basically you can
crawl
yeah something like that but it would
have made this process a lot more
difficult but the way that I found out
that this website had an API and again
we're thinking energy industry was kind
of frustrating because I had to email
their support team wait like I don't
know a week to get a response back and
they emailed me back a 60 page PDF that
was to be fair it had everything but I
had to literally go through the entire
thing to get you know okay you need to
have this type of of information in the
headers you need to like this type of
information here's the URL and whatnot
and things like that responses to expect
and all that kind of stuff so
key factor of the website had an API
finally I also knew that it was
important that the website respond like
the website response itself was
important because uh and it'd be if it
wasn't a success the file needs to be
processed again but it was important
that every single file get processed and
even if it was successful or not
successful that response was going to be
you need to save somewhere because my
colleagues were going to need to be able
to see okay oh just to verify the manual
verification there everything looks
green perfect okay so then
so let's talk about this let's in steps
so I have this key the the key facts and
I'm like okay so now I understand the
actual manual process and when you're
automating something it's very important
that you understand every single bit so
it's like think about like when you're
trying to change a light bulb it's not
like oh just change the light bulb like
no it's like step one do you have
um is it the light bulb the actual issue
like turn the light on is the light
working in other places sorry you've
figured out the light bulb do you have
an actual spare light bulb you don't
want to go up to go change your light
bulb and turns out you don't have to
spare the light bulb then you gotta go
to the store right so when you think of
them automating a manual process it's
all about every single little step and
everything that could go go right and
wrong in this expect a new one all right
so let's talk about the steps here so
first of all uh we know that we have to
get the files automatically go somewhere
once they are made available by that
existing system and we know that for
that requires the the change in that
existing system to be done by the team
that handles that system uh awesome okay
Second Step here is that I know that as
soon as a file is in that place we need
to process it
the actual process the steps and process
is obtaining the file then we need to
obtain the credential
then we need to parse the XML because
these files are XML data and that API
was expecting XML then we needed to
submit the XML with the correct
credential to the API if the API
returned a successful response we needed
to Archive the file because it's been
processed we don't need it anymore and
when you have like a hot hot storage
versus archive storage there's a
significant price reduction in storage
that you use just for archiving so the
more we can save money the better
and if the API didn't return a success
we don't move the file we have to leave
it in the same place that it is so it
can get processed again fantastic then
we need to regardless of what the
response is save the response somewhere
save the API response somewhere and also
I mentioned that we needed to be able to
update credentials and we also needed to
be able to see
um the actual responses I like this
common XML horror yes yeah I'm Marvel
fan of Json myself but hey XML exists a
lot and it's still very used in many
many places but yes it was
it would have been so much easier to
work with Json but yeah we have XML
there so anyway additional tools uh we
need to create a credential updater yes
of course because we need to interact
with this service that was going to
share there's always keep the
credentials and we needed to build some
sort of static website that my
colleagues could just visit and they go
to the website and to see all the
results and then easily see it's green
and green green if there's something red
then oh they have an idea of what needs
to get processed again maybe they kick
off the automation process or whatever
once more okay so anyway now we have all
the steps and now we talk about the plan
here again still kind of like early on
not as confident but yeah we're going
into it right so we got the plan now
things are about to get I'm warning you
all things are about to get chaotic but
that's sort of how it is when you're
sort of like you know you know putting
the process together kind of figuring
things out you're kind of doing a brain
dump there's kind of things can get a
little chaotic until like you know they
smooth it now and you actually figure
stuff out right but anyway so again
let's let's map these steps into Azure
services so we need the files to go
somewhere well
perfect solution for that would be blob
storage and like I mentioned blob
storage is essentially a serverless
storage service with Azure okay so I
could give the blob storage
um the proper like SAS or token or
however it is that you want to your
service to connect to
um I know it was like I think it was
like a net application so they were able
to just get like okay these files are
manually uh with these files are
generated we'll send them straight off
to a blob container there in Azure
storage
then I know that we need to process each
file as they come in Azure functions is
a perfect service for that Azure
functions has a bunch of triggers and
bindings triggers are ways that your
your event driven code which is what
functions are can execute can run and
then blob trigger would mean that okay
your function is expecting some sort of
change in some sort of blob container
which is the change in this case would
be oh yeah a new file is available let's
do something with it so we're going to
use functions to obtain the file
right and then the next step here is to
obtain the credential now I mentioned we
have to have a safe place safe secure uh
top-notch security uh where we can
maintain our credential but we also need
it to be somewhere that we can update in
somewhat of a streamlined way for that
we have key Vault keyboard hottest sdks
and apis that you can use to securely
interact and update with them but also
you can use an access policy within
Azure to have your Azure functions and
key Vault to talk to each other and when
we're in the solution diagram we'll see
that there's a back and forth here but
for right now this is let's think of
like the Soto code like before you
actually code you have SoDo code right
so this is like Soto diagram
um with Cloud infrastructure and whatnot
okay so then parse XML that was another
job before the function itself and we've
got it here pointing to the function now
remember I told everyone there is one
big change I would make in my updated
solution if I were to rebuild this thing
in 2022. I'm going to tell you in this
slide you can probably get a feel for
what it was but
pay attention Okay so anyway then we've
got to submit the XML with credential to
the API and again regardless of the
response we need to save that too and
then we need to
the actual submission of that of that
data will be done from that function as
well
next we've got to save the API response
we need to actually grab that data and
put it somewhere table storage is
probably like the cheapest and more
straightforward way and you can have an
output binding on the Azure function to
save to table storage so perfect like
table storage a couple of cents a month
we'll talk more about the actual
finances of the solution later on a
couple cents a month we don't need
Cosmos DB we don't need an Azure SQL
database for this like no it's a simple
table storage would do it for us
finally we need to create the additional
tools which would be a credential
updater remember we're going to have
credentials in key Vault so we need to
be able to rotate those every 90 days or
so so I built this we're going to use a
python uh well I used I guess a python
SDK to interact with key Vault and then
we needed a static site to view the
results remember that
Blazer uh I don't know if anyone has
experience with laser I'm a big fan
Blazer allows you to use.net
Technologies to develop full stack
applications you have Blazer webassembly
which runs off webassembly technology
and you're using.net for just static
websites you have Blazer server side
that has a server and a front-end
um very cool technology there but anyway
yes this is essentially a stepson Azure
right so again everything's a little
chaotic but you might be able to get a
feel for what changes I would have done
but anyway we've got essentially all the
services that we're going to work with
here okay so now at this point in my in
my in my I guess
experience or my time with this project
I've got I've sat down with my
colleagues I I got all my key facts I I
I've approached a thing I've planned it
out and I'm feeling more confident so
now we've graduated to a bit with a
laptop and ready to work in Azure what
not so we're going to dive into the
solution right so for the solution
itself that first step is let's save the
files and remember I mentioned that this
is some sort of internal application
that another engineer has built
so we need to get these files these
generated files get saved to something
and I added a note there that says it
might be a little too small but what it
says is that these files are saved using
a naming convention and this is
important for a Next Step that we have
uh coming up here but yeah I'm a
convention we're going to use blob
storage we're going to we have here
processing blob storage which are files
that were manually downloaded are now
going to go straight into blob storage
and blob storage each file there is
going to trigger an Azure function right
quick question so yes the the file that
the internal application generates
do engineers put them manually in the
blob storage
no they okay it uses
um probably like uh blob storage API or
probably SDK uh like done at SDK just
get the files to go straight into things
so that was automated as well none of
the process from it was manual unless it
was manual verification or maybe like
resubmission needed to be done again
um but that was a good question there
okay so yeah we know that as soon as a
file hits a blob storage each in each
file each it's not just like a group of
each file so we're talking about 50 60
files is going to trigger this process
XML file here too now remember that I
said there was a naming convention so
the files were being saved like for
example uh I don't know company name
Dash something dot XML that naming
convention was important because that is
what the function the function would
grab the name of the file and then go to
Kevon it's like okay keyboard return me
the the credential that matches this
naming convention so you see there's a
two-way Arrow here because key Vault
would return that credential to the
Azure function now
the reason that the credential sort of
like the grabbing the credential is the
first step here is because there is no
point in processing the actual XML or
the file if I can't find the proper
credential right because you're not
going to be able to submit it to the API
so that's the first step here so
considering everything works out we're
like okay the key vault is returning now
the credential back into the Azure
function and then the function has
everything it needs to parse the XML and
send it off to the submission website
API we're just calling it submission
website that's not the actual official
name so the function sends off that XML
data with a credential and sends it off
to the API and the API will return some
sort of response again rather it's like
successful or not successful response we
need to save the response somewhere next
that same function did you have a
question
so the credentials were file specific or
were they required as headers for the
submission website
uh yeah they were required as headers so
we would parse the the key Vault has a
key value pair and we would grab those
and send them off in the call and then
we would send the XML data with a call
as well makes sense yeah um like the
payload I guess um so then response
again we're using table storage to keep
the responses uh Table stores is a very
very simple
like you think you think of a table in a
database very very simple service
um compared to something like Cosmo ZB
or Azure SQL and whatnot so the function
will save the API response to the table
and then of course we talked about
archiving files as well the function
would then archive the file if it was a
success so this is the diagram according
to if it was a successful submission of
course now
pay attention to this we're gonna this
and I have like sort of this
rectangle here that just kind of tells
you what is built in Azure and what is
sort of on-prem so remember that
internal service was deployed on a
virtual machine that lived on-prem and
then we had everything that's inside the
blue which is indicating a resource
Group living inside of azure right so
paid digit here to sort of what you
think you would change or what you think
needs to be changed and this solution at
50 60 files runs fine but if you throw
1000 files at this if you throw several
thousand which obviously this this this
company wanted to obtain you more and
more clients and as more clients would
onboard to their catalog they would have
more and more files right so their goal
was to be able to to process these files
um you know you start to hear
performance issues with this current
solution if you get into the you know
several thousand probably several
thousands of files but anyway if anyone
has an idea in the chat let me know but
anyway now it's not just the solution
itself I tend I I mentioned that I had
also built a couple of helper tools so
the first thing was building essentially
a CLI that would interact with a key
Vault using python so you know if you
use the CLI before it was essentially
something that said uh username like
update I don't know I can't remember
what my prefix was but it was like
update key ball uh you would provide the
username and tell you okay which what's
the new password you want to do and then
it would go into key Vault and update
that
um
correctly
uh worship you want to take a guess let
me let me let me take a guess before
before I show you the updated one too
okay cool cool cool cool so that was the
first the the this was a great this is a
great experience using python using sdks
with Azure it was a very straightforward
thing but it was my first time using
python in a professional setting so it
was a lot of fun
um and it was something like pretty easy
to run to uh and then the next thing was
a static results website which I was
using Blazer web assembly on the front
end and every time you would hit and it
was deployed on app service I believe or
maybe it was even deployed on blob
storage
um just like a static site so every time
you would hit the website URL it would
go call an HTT to be triggered Azure
function that would get a a
the results of all the table and parse
it to Json and return Json back to the
site and then the site was a very simple
static site it would say a file name
success file name or or whatever the
response was and the time that it was
submitted uh and and whatnot I like
someone says Azure Q storage okay you're
getting there that's a good guess I like
that guess there it's a good guess there
so anyway
um now I want to talk a little bit just
about making my case for promotion
because
you know we work we build we upskill
because essentially we want career
advancement we want more money like
there's nothing wrong in saying these
kinds of things but earlier even before
I was in this role someone gave me some
of the best advice that I've taken in my
entire career which is like the people
who promote you don't necessarily care
about
the tech itself because the tech that
you need to be able to work with will
change like I don't know weekly monthly
yearly what not they care about how
efficient you can be and how you can
save them time and how you can save them
money so every time that I would try to
approach a a promotion I would make sure
I can talk in ways that like oh what was
the actual outcome of my project it's
not like oh I implemented containers or
I implemented serverless or I automated
something it's like what was the actual
outcome what was the actual impact oh
and we see Mike Mike drop bit here you
see this is bit dropping a microphone
it's pretty good yeah I was actually I
was I was
very proud after this project because I
had been talking to my colleagues
they're like you know we have so much
time left like this is all streamlined
that we know that this process is
working all these kinds of things I was
very confident but there's a one line
here if I had to summarize this project
was my solution frees up 18 hours of
manual work a week and runs for under
one dollar a month now I don't really
know the salary of their it was like but
if we you know estimate that they were
making I don't know fifty dollars an
hour you know it averages about two
thousand dollars a month you're
comparing a thousand dollars a month to
a solution that runs under a dollar to
be exact the solution was running for 30
cents a a month but I just put a dollar
because a little bit more catchier here
but
um yeah for 30 cents and the functions
execution was free because you have a
grant I think per subscription with
functions I think it's monthly of a
million executions so the compute is
completely free this was just storage
and if I was probably a little bit more
clever with the storage I'd probably say
but you know 30 cents compared to a
thousand dollars a month massive
difference and after this I was promoted
and then I spent a whole another year as
a junior college Union at this at this
at this company and got a lot more
Hands-On Azure and years later I'm here
working at Microsoft's but anyway let's
dive into the fun part I wanted to see
someone said either that or service bus
so Richard did you want to take a guess
yeah I was going to say some kind of
cooling service okay yes queuing queuing
messaging so when we think of um
distributed systems which cloud is all
about like the more distributed that you
can make it the more Advantage you can
take it of different services in in the
cloud and whatnot
um you're gonna get the best bang for
your buck and one so yes
we are talking about having a messaging
service and I would add another function
because that that initial function was
doing a little bit too much so I would
now abstract away the the archiving so
the actual moving of the files and the
saving of the response to table source
to another function and that way that
initial function can scale to process as
many files as it needs to without
hitting like any performance issues so
in this case I have event grids
um it I think Azure has event grids
you've got
um I can't remember we have like four
different messaging services I know
someone mentioned service bus uh Q
storage as well but it you know it
really kind of depends on like what
you're doing like aiming for cues versus
events and messages versus events and
whatnot events tend to work a lot better
with serverless and whatnot so I would
have
um
event grid here and then you can trigger
an Azure function with event grid so
when there's a new event and you can
send like information with it so file
name and then response and then that
function would archive and then save
response I could probably do
another function that so it was just
like one archives and one does the
saving the response to table storage but
that was probably a little bit too
Overkill but this would be the updated
version of my solution uh and if I
actually implemented it in 2022